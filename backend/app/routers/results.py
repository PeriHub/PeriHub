# SPDX-FileCopyrightText: 2023 PeriHub <https://github.com/PeriHub/PeriHub>
#
# SPDX-License-Identifier: Apache-2.0

import csv
import os
import shutil
from typing import Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from exodusreader import exodusreader
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import FileResponse, JSONResponse

from ..support.base_models import ResponseModel
from ..support.file_handler import FileHandler
from ..support.globals import dev, log, max_nodes
from ..support.results.analysis import Analysis
from ..support.results.crack_analysis import CrackAnalysis

router = APIRouter(prefix="/results", tags=["Results Methods"])


@router.get(
    "/getFractureAnalysis",
    operation_id="get_fracture_analysis",
    response_class=FileResponse,
    responses={
        200: {
            "description": "The image.",
            "content": {"image/png": {"schema": {"type": "string", "format": "binary"}}},
        }
    },
)
def get_fracture_analysis(
    model_name: str = "Dogbone",
    model_folder_name: str = "Default",
    height: float = 10,
    crack_length: float = 17.5,
    young_modulus: float = 5000,
    poissions_ratio: float = 0.33,
    yield_stress: float = 74,
    cluster: bool = False,
    tasks: int = 32,
    output: str = "Output1",
    step: int = -1,
    request: Request = "",
):
    """doc"""
    username = FileHandler.get_user_name(request, dev)

    if not FileHandler.copy_results_from_cluster(
        username, model_name, model_folder_name, cluster, False, tasks, output
    ):
        raise IOError  # NotFoundException(name=model_name)

    resultpath = FileHandler.get_local_model_folder_path(username, model_name, model_folder_name)
    file = os.path.join(resultpath, model_name + "_" + output + ".e")

    file_name, filepath = CrackAnalysis.write_nodemap(file, step)

    crack_coordinate = CrackAnalysis.get_crack_end(file, step)

    filepath = CrackAnalysis.fracture_analysis(
        model_name,
        height,
        crack_coordinate,
        young_modulus,
        poissions_ratio,
        yield_stress,
        file_name,
        filepath,
    )

    return FileResponse(filepath, media_type="image/png")


@router.get("/getEnergyReleasePlot", operation_id="get_energy_release_plot")
def get_energy_release_plot(
    model_name: str = "Dogbone",
    model_folder_name: str = "Default",
    cluster: bool = False,
    output_exodus: str = "Output1",
    output_csv: str = "Output2",
    tasks: int = 32,
    force_output_name: str = "External_Forcey",
    displacement_output_name: str = "External_Displacementy",
    step: int = -1,
    thickness: float = None,
    # crack_start: float = 56.25,
    request: Request = "",
):
    """doc"""
    username = FileHandler.get_user_name(request, dev)

    if not FileHandler.copy_results_from_cluster(
        username, model_name, model_folder_name, cluster, False, tasks, output_exodus
    ):
        raise IOError  # NotFoundException(name=model_name)
    if not FileHandler.copy_results_from_cluster(
        username, model_name, model_folder_name, cluster, False, tasks, output_csv
    ):
        raise IOError  # NotFoundException(name=model_name)

    resultpath = FileHandler.get_local_model_folder_path(username, model_name, model_folder_name)
    file = os.path.join(resultpath, model_name + "_" + output_csv + ".csv")
    result_file = os.path.join(resultpath, model_name + "_" + output_csv + ".png")

    if not os.path.exists(file):
        return ResponseModel(data=False, message=model_name + "_" + output_csv + ".csv can not be found")

    df = pd.read_csv(file)

    file = os.path.join(resultpath, model_name + "_" + output_exodus + ".e")
    (crack_length, crack_width, time) = CrackAnalysis.get_crack_length(file, step)

    x = df[displacement_output_name]
    y1 = df[force_output_name]

    time_array = df["Time"]
    index = np.argmin(np.abs(time_array - time))
    y2 = np.linspace(0, y1[index], index)

    plt.clf()
    plt.plot(x, df[force_output_name], label="Original data")

    x = x[:index]
    y1 = y1[:index]

    plt.plot(x, y2, "r", label="Linear equation (y=x+2)")
    plt.fill_between(x, y1, y2, alpha=0.3, label="Area between lines")

    z = np.array(y1 - y2)
    dx = x[2] - x[1]
    areas_pos = abs(z[:-1] + z[1:]) * 0.5 * dx
    dissipated_energy = np.sum(areas_pos)

    thickness_crack = thickness
    if thickness == None:
        thickness_crack = crack_width

    GIC = dissipated_energy / (thickness_crack * crack_length)

    # Add title and labels
    plt.title(f"Energy: {round(dissipated_energy,4)} | Crack: {round(crack_length,4)}mm | GIC: {round(GIC,4)}N/mm")
    plt.xlabel("Displacement [mm]")
    plt.ylabel("Force [N]")
    plt.legend()

    # Display the plot
    plt.savefig(result_file)

    return FileResponse(result_file, media_type="image/png")


@router.get("/getEnfAnalysis", operation_id="get_enf_analysis")
def get_enf_analysis(
    model_name: str = "ENFmodel",
    model_folder_name: str = "Default",
    length: float = 15.0,
    width: float = 1.0,
    crack_length: float = 6.0,
    cluster: bool = False,
    tasks: int = 32,
    output: str = "Output1",
    step: int = -1,
    request: Request = "",
):
    """doc"""
    username = FileHandler.get_user_name(request, dev)

    if not FileHandler.copy_results_from_cluster(
        username, model_name, model_folder_name, cluster, False, tasks, output
    ):
        raise IOError  # NotFoundException(name=model_name)

    resultpath = FileHandler.get_local_model_folder_path(username, model_name, model_folder_name)
    file = os.path.join(resultpath, model_name + "_" + output + ".e")

    g2c = CrackAnalysis.get_g2c(file, length, width, crack_length, step)

    try:
        return g2c
    except IOError:
        log.error("%s results can not be found on %s", model_name, cluster)
        return model_name + " results can not be found on " + cluster


@router.get("/getPlot", operation_id="get_plot")
def get_plot(
    model_name: str = "Dogbone",
    model_folder_name: str = "Default",
    cluster: bool = False,
    output: str = "Output1",
    tasks: int = 32,
    # x_variable: str = "Time",
    # x_axis: str = "X",
    # x_absolute: bool = True,
    # y_variable: str = "External_Displacement",
    # y_axis: str = "X",
    # y_absolute: bool = True,
    request: Request = "",
):
    """doc"""
    username = FileHandler.get_user_name(request, dev)

    if not FileHandler.copy_results_from_cluster(
        username, model_name, model_folder_name, cluster, False, tasks, output
    ):
        raise IOError  # NotFoundException(name=model_name)

    resultpath = FileHandler.get_local_model_folder_path(username, model_name, model_folder_name)
    file = os.path.join(resultpath, model_name + "_" + output + ".csv")

    if not os.path.exists(file):
        return ResponseModel(data=False, message=model_name + "_" + output + ".csv can not be found")

    # x_data = Analysis.get_global_data(file, x_variable, x_axis, x_absolute)
    # y_data = Analysis.get_global_data(file, y_variable, y_axis, y_absolute)

    data = {}
    first_row = True
    # try:
    with open(file, "r", encoding="UTF-8") as csv_file:
        reader = csv.reader(csv_file)
        for row in reader:
            if first_row:
                # Extract column names from the first row
                column_names = row
                for column_name in column_names:
                    data[column_name] = []

                first_row = False
            else:
                # Populate data dictionary with values
                for i, value in enumerate(row):
                    data[column_names[i]].append(value)

    return ResponseModel(data=data, message="Plot received")
    # except IOError:
    #     log.error("%s results can not be found on %s", model_name, cluster)
    #     return ResponseModel(data=data, message=model_name + " results can not be found on " + cluster)


@router.get("/getResults", operation_id="get_results")
def get_results(
    model_name: str = "Dogbone",
    model_folder_name: str = "Default",
    output: str = "Output1",
    tasks: int = 32,
    cluster: bool = False,
    all_data: bool = False,
    request: Request = "",
):
    """doc"""
    username = FileHandler.get_user_name(request, dev)

    if not FileHandler.copy_results_from_cluster(
        username, model_name, model_folder_name, cluster, all_data, tasks, output
    ):
        raise HTTPException(
            status_code=404,
            detail=model_name + " results can not be found on",
        )

    # resultpath = './simulations/' + os.path.join(username, model_name)
    resultpath = FileHandler.get_local_model_path(username, model_name)
    zip_file = os.path.join(resultpath, model_name + "_" + model_folder_name)

    # check if folder contains only one .e file
    if not all_data:
        for file in os.listdir(os.path.join(resultpath, model_folder_name)):
            if file.endswith(".e"):
                return FileResponse(os.path.join(resultpath, model_folder_name, file))

    try:
        print(zip_file)
        shutil.make_archive(zip_file, "zip", os.path.join(resultpath, model_folder_name))

        response = FileResponse(
            zip_file + ".zip",
            media_type="application/x-zip-compressed",
        )
        response.headers["Content-Disposition"] = (
            "attachment; filename=" + model_name + "_" + model_folder_name + ".zip"
        )
        # return StreamingResponse(iterfile(), media_type="application/x-zip-compressed")
        return response
    except IOError:
        log.error("%s results can not be found on %s", model_name, cluster)
        return model_name + " results can not be found"


def get_cell_data(variable, points, point_data, cell_data, block_data, displ_factor):
    np_points_all_x = np.array([])
    np_points_all_y = np.array([])
    np_points_all_z = np.array([])

    cell_value = np.array([])

    for block_id in range(0, len(block_data)):
        block_ids = block_data[block_id][:, 0]

        block_points = points[block_ids]

        np_first_points_x = np.array(block_points[:, 0])
        np_first_points_y = np.array(block_points[:, 1])
        np_first_points_z = np.array(block_points[:, 2])

        if "Displacements" in point_data:
            np_displacement_x = np.array(point_data["Displacements"][block_ids, 0])
            np_displacement_y = np.array(point_data["Displacements"][block_ids, 1])
            np_displacement_z = np.array(point_data["Displacements"][block_ids, 2])

            np_points_x = np.add(
                np_first_points_x,
                np.multiply(np_displacement_x, displ_factor),
            )
            np_points_y = np.add(
                np_first_points_y,
                np.multiply(np_displacement_y, displ_factor),
            )
            np_points_z = np.add(
                np_first_points_z,
                np.multiply(np_displacement_z, displ_factor),
            )
        else:
            np_points_x = np_first_points_x
            np_points_y = np_first_points_y
            np_points_z = np_first_points_z

        np_points_all_x = np.concatenate([np_points_all_x, np_points_x])
        np_points_all_y = np.concatenate([np_points_all_y, np_points_y])
        np_points_all_z = np.concatenate([np_points_all_z, np_points_z])

        if variable == "Block":
            cell_value = np.concatenate(
                [
                    cell_value,
                    np.full_like(np_points_x, block_id),
                ]
            )
        else:
            if block_id in cell_data[variable][0] and max(cell_data[variable][0][block_id]) > 0:
                cell_value = np.concatenate(
                    [
                        cell_value,
                        cell_data[variable][0][block_id],
                    ]
                )
            else:
                cell_value = np.concatenate([cell_value, np.full_like(np_points_x, 0)])

    return np_points_all_x, np_points_all_y, np_points_all_z, cell_value


def get_point_data(variable, axis, displ_factor, use_multi_data, points, point_data):
    np_first_points_x = np.array(points[:, 0])
    np_first_points_y = np.array(points[:, 1])
    np_first_points_z = np.array(points[:, 2])

    np_points_all_z = None

    try:
        np_displacement_x = np.array(point_data["Displacementsx"])
        np_displacement_y = np.array(point_data["Displacementsy"])
        try:
            np_displacement_z = np.array(point_data["Displacementsz"])
            np_points_all_z = np.add(
                np_first_points_z,
                np.multiply(np_displacement_z, displ_factor),
            )
        except:
            pass

        np_points_all_x = np.add(
            np_first_points_x,
            np.multiply(np_displacement_x, displ_factor),
        )
        np_points_all_y = np.add(
            np_first_points_y,
            np.multiply(np_displacement_y, displ_factor),
        )

    except Exception:
        print("No Displacements")
        np_points_all_x = np_first_points_x
        np_points_all_y = np_first_points_y
        np_points_all_z = np_first_points_z

    cell_value = []
    if axis == "Magnitude" and use_multi_data:
        cell_value_x = point_data[variable + "x"]
        cell_value_y = point_data[variable + "y"]
        cell_value = np.sqrt(cell_value_x**2 + cell_value_y**2)
    else:
        if point_data.keys().__contains__(variable):
            cell_value = point_data[variable]
        else:
            cell_value = np.full_like(np_points_all_x, 0)

    return np_points_all_x, np_points_all_y, np_points_all_z, cell_value


@router.get("/getPointDataResults", operation_id="get_point_data_results")
def get_data(
    model_name: str = "Dogbone",
    model_folder_name: str = "Default",
    output: str = "Output1",
    tasks: int = 32,
    cluster: bool = False,
    axis: str = "Magnitude",
    step: int = 78,
    displ_factor: float = 100,
    variable: str = "Displacements",
    filter: str = "",
    color_bar_min: float = None,
    color_bar_max: float = None,
    request: Request = "",
):
    """doc"""
    username = FileHandler.get_user_name(request, dev)

    if not FileHandler.copy_results_from_cluster(
        username, model_name, model_folder_name, cluster, False, tasks, output
    ):
        raise IOError  # NotFoundException(name=model_name)

    resultpath = FileHandler.get_local_model_folder_path(username, model_name, model_folder_name)
    file = os.path.join(resultpath, model_name + "_" + output + ".e")

    if not os.path.exists(file):
        return ResponseModel(
            data=False,
            message="Results can not be found, maybe they are not generated yet.",
        )

    number_of_steps = exodusreader.get_number_of_steps(file)

    try:
        (
            points,
            point_data,
            global_data,
            cell_data,
            ns,
            block_data,
            time,
        ) = exodusreader.read_timestep(file, step)

    except IndexError:
        try:
            (
                points,
                point_data,
                global_data,
                cell_data,
                ns,
                block_data,
                time,
            ) = exodusreader.read_timestep(file, number_of_steps)
        except IndexError:
            return ResponseModel(
                data=False,
                message="Results can not be found, maybe they are not generated yet.",
            )

    use_cell_data = False

    variable_list = list(point_data.keys())
    variable_list = list(
        set([entry[:-1] if entry[-1].lower() in ["x", "y", "z"] else entry for entry in variable_list])
    )

    np_points_all_z = None
    use_multi_data = False

    if variable in [
        "Displacements",
        "Forces",
        "Strainx",
        "Strainy",
        "Strainz",
        "Cauchy Stressx",
        "Cauchy Stressy",
        "Cauchy Stressz",
    ]:
        use_multi_data = True
    axis_id = 0

    if axis == "X":
        axis_id = 0
        if use_multi_data:
            variable = variable + "x"
    elif axis == "Y":
        axis_id = 1
        if use_multi_data:
            variable = variable + "y"
    elif axis == "Z":
        axis_id = 2
        if use_multi_data:
            variable = variable + "z"
    elif axis == "Magnitude":
        axis_id = 0

    filter_value = []
    use_filter = len(filter) > 0

    if use_cell_data:
        np_points_all_x, np_points_all_y, np_points_all_z, cell_value = get_cell_data(
            points, point_data, cell_data, block_data, axis_id, variable
        )

    else:
        np_points_all_x, np_points_all_y, np_points_all_z, cell_value = get_point_data(
            variable, axis, displ_factor, use_multi_data, points, point_data
        )
        if use_filter:
            _, _, _, filter_value = get_point_data(
                filter,
                "Not_Magnitude",
                displ_factor,
                use_multi_data,
                points,
                point_data,
            )

    if np_points_all_z is None:
        np_points_all_z = np.zeros_like(np_points_all_x)

    if use_filter:
        cell_value = cell_value[filter_value != 0]
        np_points_all_x = np_points_all_x[filter_value != 0]
        np_points_all_y = np_points_all_y[filter_value != 0]
        np_points_all_z = np_points_all_z[filter_value != 0]

    if len(cell_value) == 0:
        return ResponseModel(data=False, message="All nodes are filtered out. Remove filter or change time value.")

    min_cell_value = np.min(cell_value) if color_bar_min == None else color_bar_min
    max_cell_value = np.max(cell_value) if color_bar_max == None else color_bar_max
    if max_cell_value == min_cell_value:
        normalized_cell_value = np.zeros_like(cell_value)
    else:
        normalized_cell_value = (cell_value - min_cell_value) / (max_cell_value - min_cell_value)
    print(time)

    reduce_factor = 1
    if len(np_points_all_x) > max_nodes:
        reduce_factor = int(len(np_points_all_x) / max_nodes)
        log.info(f"Number of nodes in file is too large, only every {reduce_factor}th node is read!")

    data = {
        "nodes": np.ravel(
            [np_points_all_x[::reduce_factor], np_points_all_y[::reduce_factor], np_points_all_z[::reduce_factor]],
            order="F",
        ).tolist(),
        "value": normalized_cell_value.tolist()[::reduce_factor],
        "variables": variable_list,
        "number_of_steps": number_of_steps,
        "min_value": min_cell_value,
        "max_value": max_cell_value,
        "time": np.format_float_scientific(time, 2),
    }

    return JSONResponse(content=data)
